/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/ymax29os/.local/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/ymax29os/.local/lib/python3.9/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch [1/50] 	 Discriminator Loss: 0.0817 	 Generator Loss: 0.1007
Epoch [2/50] 	 Discriminator Loss: 0.0851 	 Generator Loss: 0.0953
Epoch [3/50] 	 Discriminator Loss: 0.0889 	 Generator Loss: 0.0994
Traceback (most recent call last):
  File "/home/ymax29os/ams_project-2/transformer_GAN/main.py", line 52, in <module>
    generator = train_transformer_model(
  File "/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py", line 133, in train_transformer_model
    output = discriminator(fake_data).view(-1)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py", line 82, in forward
    out = layer(out, out, out)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ymax29os/ams_project-2/transformer_GAN/transformer_model.py", line 23, in forward
    attention = self.attention(query, key, value)[0]
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/functional.py", line 5528, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p)
  File "/home/ymax29os/.local/lib/python3.9/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt
